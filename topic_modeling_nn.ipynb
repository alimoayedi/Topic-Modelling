{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alimoayedi/Topic-Modelling/blob/main/topic_modeling_nn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ombtYSSrAFvC",
        "outputId": "c7c236a8-9c1c-4cc2-9cbe-8830a072837f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"
          ]
        }
      ],
      "source": [
        "%reset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPlHTYA05qWL"
      },
      "source": [
        "# Importing All Required Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "vUA1gXMb5qr9",
        "outputId": "09c80cdd-dfb2-4556-e801-f25c596c5c2b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2024-06-28 15:03:54--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2024-06-28 15:03:54--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2024-06-28 15:03:55--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  5.02MB/s    in 2m 39s  \n",
            "\n",
            "2024-06-28 15:06:34 (5.18 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n",
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import utilities as ut\n",
        "import nltk\n",
        "import random\n",
        "\n",
        "from LoadDataset import LoadReutersDataset\n",
        "\n",
        "from nltk import pos_tag\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.svm import LinearSVR\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "from tensorflow.keras.preprocessing import text, sequence\n",
        "\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Embedding, LSTM, Dense, concatenate, Flatten\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Download GloVe embeddings\n",
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove.6B.zip\n",
        "\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGSA5l1mN2I7"
      },
      "source": [
        "# Import functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "1KywX9DLkxRV"
      },
      "outputs": [],
      "source": [
        "# Define the function to remove duplicates from a list\n",
        "def remove_duplicates_terms(doc):\n",
        "    return list(set(doc))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "4CxS5gNKcI5R"
      },
      "outputs": [],
      "source": [
        "# join tokens to make a string\n",
        "def join_tokens(tokens):\n",
        "   return \" \".join(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "OSgMj0-Tm68n"
      },
      "outputs": [],
      "source": [
        "# replace topics with their indexes, unfound topics are replaced by zero!\n",
        "def replace_with_index(topic_lst):\n",
        "    return [favorite_topics.index(topic) if topic in favorite_topics else 99 for topic in topic_lst]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "blZP0XPooUgo"
      },
      "outputs": [],
      "source": [
        "# remove zeros from a list of topics' indexes\n",
        "def remove_zeros(topic_lst):\n",
        "    return [topic for topic in topic_lst if topic != 99]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "mNtveieJJEe8"
      },
      "outputs": [],
      "source": [
        "def vectorize(vocab_vec, tokens_lst):\n",
        "    return [vocab_vec[term] for term in tokens_lst if term in vocab_vec]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ESZVrj9v9rId"
      },
      "outputs": [],
      "source": [
        "def remove_high_correlated_tokens(cosine_sim_score, tokens_lst):\n",
        "    to_remove = set()\n",
        "    for term_1 in tokens_lst:\n",
        "        for term_2 in tokens_lst:\n",
        "            if term_1 != term_2 and cosine_sim_score[term_1, term_2] > 0.9:\n",
        "                    to_remove.add(term_2)\n",
        "    return [term for term in tokens_lst if term not in to_remove]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "5wX1Cw0x0z9F"
      },
      "outputs": [],
      "source": [
        "def lemmatize(tokenized_text):\n",
        "    # Perform POS tagging\n",
        "    pos_tags = nltk.pos_tag(tokenized_text)\n",
        "\n",
        "    # Initialize WordNetLemmatizer\n",
        "    lemmatizer = nltk.WordNetLemmatizer()\n",
        "\n",
        "    # Lemmatize words using POS tags\n",
        "    lemmas = []\n",
        "    for word, pos_tag in pos_tags:\n",
        "        # Map POS tags to WordNet tags\n",
        "        if pos_tag.startswith('N'):\n",
        "            wn_tag = 0  # Noun\n",
        "        elif pos_tag.startswith('V'):\n",
        "            wn_tag = 1  # Verb\n",
        "        elif pos_tag.startswith('J'):\n",
        "            wn_tag = 2  # Adjective\n",
        "        elif pos_tag.startswith('R'):\n",
        "            wn_tag = 3  # Adverb\n",
        "        else:\n",
        "            wn_tag = 4  # No specific tag\n",
        "\n",
        "        # Lemmatize the word with WordNet\n",
        "        lemmas.append(wn_tag)\n",
        "\n",
        "    return lemmas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "a3uNqIt5FxQK"
      },
      "outputs": [],
      "source": [
        "def white_space_splitter(text):\n",
        "    return text.split()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "mIOzCiXhEcnC"
      },
      "outputs": [],
      "source": [
        "def padding(lst, maximum_length):\n",
        "    return sequence.pad_sequences([lst], maxlen=maximum_length, padding='post')[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Yi7KdwhrOIMS"
      },
      "outputs": [],
      "source": [
        "def make_tuple(lst, size):\n",
        "    list_size = len(lst)\n",
        "    if size == 2:\n",
        "        return [(lst[index], lst[index+1]) for index in range(list_size - size + 1)]\n",
        "    elif size == 3:\n",
        "        return [(lst[index], lst[index+1], lst[index+2]) for index in range(list_size - size + 1)]\n",
        "    else:\n",
        "        raise ValueError(\"Error in the value of the size! Check the method!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "6rEOrvH5DAb8"
      },
      "outputs": [],
      "source": [
        "def get_unique_token_pairs(lst_docs):\n",
        "    unique_pairs = []\n",
        "    for doc in lst_docs:\n",
        "        unique_pairs.extend(list(set(doc)))\n",
        "    return list(set(unique_pairs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "5lrOtaakuPNW"
      },
      "outputs": [],
      "source": [
        "def get_token_pairs_count(token_pair, lst):\n",
        "    if token_pair in lst:\n",
        "        return lst.count(token_pair)\n",
        "    else:\n",
        "        return 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "O4znn6QgU65D"
      },
      "outputs": [],
      "source": [
        "# create a list of elements from columns of DF\n",
        "def create_list(row):\n",
        "    elements = row.iloc[:].tolist()\n",
        "    return elements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "8X5y-2Pd5LZO"
      },
      "outputs": [],
      "source": [
        "\n",
        "def feature_selection_rfe(X_train, y_train):\n",
        "    # Create the SVR estimator\n",
        "    svr_estimator = LinearSVR(max_iter = 10000)\n",
        "\n",
        "    # Create the RFE object with desired estimator and number of features to select\n",
        "    selector = RFE(estimator=svr_estimator, n_features_to_select=0.1, step=100)\n",
        "\n",
        "    # Fit the RFE on the training data\n",
        "    selector.fit(X_train, y_train)\n",
        "\n",
        "    # Extract the selected features based on RFE rankings\n",
        "    feature_ranks = selector.ranking_\n",
        "\n",
        "    # Get the selected features based on model performance\n",
        "    selected_features = X_train.columns[selector.support_]\n",
        "\n",
        "    return selected_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "WuZ-6RvOLfNV"
      },
      "outputs": [],
      "source": [
        "def get_number_of_tokens(df_col):\n",
        "    all_items = [item for sublist in df_col for item in sublist]\n",
        "\n",
        "    # Step 2: Convert to a set to find unique items\n",
        "    unique_items = set(all_items)\n",
        "\n",
        "    # Step 3: Count the number of unique items\n",
        "    return len(unique_items)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "UMPu3AEWzR3c"
      },
      "outputs": [],
      "source": [
        "def concatenate_arrays(columns, row):\n",
        "    concatinated = np.array([])\n",
        "    for col in columns:\n",
        "        concatinated = np.append(concatinated, row[col])\n",
        "    return concatinated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "26bqEovWGjS8"
      },
      "outputs": [],
      "source": [
        "def GloVe_embedding(word_index_dict, vocab_size, embedding_dim):\n",
        "\n",
        "    embedding_dim = 100\n",
        "    embedding_file = 'glove.6B.100d.txt'\n",
        "    embeddings_index = {}\n",
        "\n",
        "    with open(embedding_file) as f:\n",
        "        for line in f:\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            coefs = np.asarray(values[1:], dtype='float32')\n",
        "            embeddings_index[word] = coefs\n",
        "\n",
        "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "    for word, index in word_index_dict.items():\n",
        "        if index < vocab_size:\n",
        "            embedding_vector = embeddings_index.get(word)\n",
        "            if embedding_vector is not None:\n",
        "                embedding_matrix[index] = embedding_vector\n",
        "\n",
        "    return embedding_matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tuwZdtYm4ffq"
      },
      "source": [
        "# **Import Data**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h0sbrqiZyqre"
      },
      "source": [
        "## Import ag_news"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ESVvlvhyqGW"
      },
      "outputs": [],
      "source": [
        "dataset_path = '/content/'\n",
        "train = pd.read_parquet(dataset_path + 'train.parquet')\n",
        "test = pd.read_parquet(dataset_path + 'test.parquet')\n",
        "\n",
        "train.columns = ['doc', 'label']\n",
        "test.columns = ['doc', 'label']\n",
        "train.index.name = 'index'\n",
        "test.index.name = 'index'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JY7Zxi0wP8V_"
      },
      "outputs": [],
      "source": [
        "documents = pd.DataFrame(train['doc'], index=train.index, columns=['doc'])\n",
        "topics = pd.DataFrame(train['label'], index=train.index, columns=['label'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NK77MgjTiDPX"
      },
      "outputs": [],
      "source": [
        "test_documents = pd.DataFrame(test['doc'], index=test.index, columns=['doc'])\n",
        "test_topics = pd.DataFrame(test['label'], index=test.index, columns=['label'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gyDYe-MKyrOA"
      },
      "source": [
        "## Import reuters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7FQTRUx55bqF"
      },
      "outputs": [],
      "source": [
        "dataset_path = '/content/drive/MyDrive/ColabNotebooks'\n",
        "loader = LoadReutersDataset(data_path=dataset_path + '/reuters21578')\n",
        "documents_dic, topics_dic, _, _, _, _, _ = loader.load()\n",
        "\n",
        "documents = pd.DataFrame.from_dict(documents_dic, orient='index', columns=['doc'])\n",
        "topics = pd.DataFrame.from_dict(topics_dic, orient='index')\n",
        "\n",
        "# # If you want to name the index, you can set the index name\n",
        "documents.index.name = 'index'\n",
        "topics.index.name = 'index'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QmcFLPx8U_0e"
      },
      "source": [
        "# **Filter Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HHQQyld2U_bY"
      },
      "outputs": [],
      "source": [
        "# remove all the documents without any specific topic\n",
        "documents = documents[topics.notna().any(axis=1)]\n",
        "topics = topics[topics.notna().any(axis=1)]\n",
        "\n",
        "# filter documents and keep only the ones with favorite topics\n",
        "# favorite_topics = ['acq', 'money-fx', 'grain', 'crude', 'trade', 'interest', 'ship', 'wheat', 'corn', 'oilseed']\n",
        "favorite_topics = ['acq', 'corn', 'crude', 'earn']\n",
        "documents = documents[topics.isin(favorite_topics).any(axis=1)]\n",
        "topics = topics[topics.isin(favorite_topics).any(axis=1)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vBn_8r789YT3"
      },
      "source": [
        "# **Sampling**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z2yMG3VO9XUK"
      },
      "outputs": [],
      "source": [
        "rand_sample = 2000\n",
        "documents = pd.DataFrame(documents.sample(n=rand_sample, random_state=42, replace=False))\n",
        "topics = pd.DataFrame(topics.loc[documents.index])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jFFE6HaN2Zol"
      },
      "outputs": [],
      "source": [
        "# num_samples = 2000\n",
        "# topics_count = []\n",
        "# for topic in favorite_topics:\n",
        "#     related_topic_doc_index = list[topics.index[topics.applymap(lambda x: x == 'acq').any(axis=1)]]\n",
        "#     num_samples * len(related_topic_doc_index) / len(topics)\n",
        "#     topics_count.append()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TAiG1vSQ-GlM"
      },
      "source": [
        "# Save Random Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ew-Z7lF-F_2"
      },
      "outputs": [],
      "source": [
        "# documents.to_hdf('/content/drive/My Drive/4topics_experiment.h5', key='documents', mode='w')\n",
        "# topics.to_hdf('/content/drive/My Drive/4topics_experiment.h5', key='topics', mode='a')\n",
        "\n",
        "# documents.to_csv('/content/drive/My Drive/sampled_documents.csv', index=True)\n",
        "# topics.to_csv('/content/drive/My Drive/sampled_topics.csv', index=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rCxMcty3_XKr"
      },
      "source": [
        "# Load Sampled Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ei1wLf_w_W9x"
      },
      "outputs": [],
      "source": [
        "documents = pd.read_hdf('/content/drive/My Drive/4topics_experiment.h5', key='documents')\n",
        "topics = pd.read_hdf('/content/drive/My Drive/4topics_experiment.h5', key='topics')\n",
        "\n",
        "\n",
        "# documents = pd.read_csv('/content/drive/My Drive/sampled_documents.csv')\n",
        "# documents.set_index('index', inplace=True)\n",
        "\n",
        "# topics = pd.read_csv('/content/drive/My Drive/sampled_topics.csv')\n",
        "# topics.set_index('index', inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NEl0CPCM2WdU"
      },
      "source": [
        "# **Filter Dataset**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kw8obdfhLNXt"
      },
      "source": [
        "# Pre-process ag_news"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nlzlWS5sLOHD"
      },
      "outputs": [],
      "source": [
        "documents['preprocess'] = documents['doc'].apply(ut.tokenize)\n",
        "\n",
        "# drop preprocessed documents with length less than 6\n",
        "topics = topics[documents['preprocess'].str.len() > 6]\n",
        "documents = documents[documents['preprocess'].str.len() > 6]\n",
        "\n",
        "favorite_topics = [0, 1, 2, 3]\n",
        "\n",
        "# remove duplicate terms from each document\n",
        "# documents['preprocess'] = documents['preprocess'].apply(remove_duplicates_terms)\n",
        "\n",
        "#join preprocced tokens to make a string. used in tf-idf and cosine scoring.\n",
        "documents['joined_tokens'] = documents['preprocess'].apply(join_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lM5kHZiLhtYe"
      },
      "outputs": [],
      "source": [
        "test_documents['preprocess'] = test_documents['doc'].apply(ut.tokenize)\n",
        "test_documents['joined_tokens'] = test_documents['preprocess'].apply(join_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dp_AzMU4LOkl"
      },
      "source": [
        "# Pre-process reuters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JT9pbguLxUYF"
      },
      "outputs": [],
      "source": [
        "# preprocess data by tokenization\n",
        "documents['preprocess'] = documents['doc'].apply(ut.tokenize)\n",
        "\n",
        "# drop preprocessed documents with length less than 6\n",
        "topics = topics[documents['preprocess'].str.len() > 6]\n",
        "documents = documents[documents['preprocess'].str.len() > 6]\n",
        "\n",
        "# remove duplicate terms from each document\n",
        "# documents['preprocess'] = documents['preprocess'].apply(remove_duplicates_terms)\n",
        "\n",
        "#join preprocced tokens to make a string. used in tf-idf and cosine scoring.\n",
        "documents['joined_tokens'] = documents['preprocess'].apply(join_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "757dPthIOnqu"
      },
      "source": [
        "# Hyper-parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4gC-v5FSOm5U"
      },
      "outputs": [],
      "source": [
        "maximum_length = 150\n",
        "embedding_dim = 100\n",
        "num_extra_features = 150\n",
        "num_classes = len(favorite_topics)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ALHuYuZN14eM"
      },
      "source": [
        "# Coding Labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "22BOj46ilnNK"
      },
      "outputs": [],
      "source": [
        "# combine all the topics into a list\n",
        "topics['topics_lst'] = topics.iloc[:, :].apply(lambda row: list(row), axis=1)\n",
        "\n",
        "# replace topics with their indexes, unfound topics are replaced by zero\n",
        "topics['topics_lst'] = topics['topics_lst'].apply(replace_with_index)\n",
        "\n",
        "# Apply the function to column 'Column'\n",
        "topics['topics_lst'] = topics['topics_lst'].apply(remove_zeros)\n",
        "\n",
        "# convert labels into a one-hot coding\n",
        "topics['one_hot'] = [list(np.sum(to_categorical(label, num_classes=num_classes), axis=0)) for label in topics['topics_lst']]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-GKyjGnjbaF"
      },
      "source": [
        "# Coding Labels (AG_NEWS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gg0gh4A-i6aF"
      },
      "outputs": [],
      "source": [
        "# combine all the topics into a list\n",
        "test_topics['topics_lst'] = test_topics.iloc[:, :].apply(lambda row: list(row), axis=1)\n",
        "\n",
        "# replace topics with their indexes, unfound topics are replaced by zero\n",
        "test_topics['topics_lst'] = test_topics['topics_lst'].apply(replace_with_index)\n",
        "\n",
        "# Apply the function to column 'Column'\n",
        "test_topics['topics_lst'] = test_topics['topics_lst'].apply(remove_zeros)\n",
        "\n",
        "# convert labels into a one-hot coding\n",
        "test_topics['one_hot'] = [list(np.sum(to_categorical(label, num_classes=num_classes), axis=0)) for label in test_topics['topics_lst']]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DCGH0axcBbVo"
      },
      "source": [
        "# **Split Data into Train and Validation, and Test set**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YuGxOq79joCs"
      },
      "source": [
        "# AG_NEWS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        },
        "id": "0Xh-us1qjl4v",
        "outputId": "48423701-f702-48ba-e736-cb804511065e"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'test_documents' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-34255a31215c>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtrainDocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalDocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainTopics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalTopics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtestDocs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_documents\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mtestTopics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_topics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainDocs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'test_documents' is not defined"
          ]
        }
      ],
      "source": [
        "rand = random.randint(10,99)\n",
        "\n",
        "trainDocs, valDocs, trainTopics, valTopics = train_test_split(documents, topics, test_size=0.2, random_state=rand)\n",
        "testDocs = test_documents\n",
        "testTopics = test_topics\n",
        "print(trainDocs.shape)\n",
        "print(trainTopics.shape)\n",
        "print(valDocs.shape)\n",
        "print(valTopics.shape)\n",
        "print(testDocs.shape)\n",
        "print(testTopics.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iklnNbA8jomV"
      },
      "source": [
        "# Reuters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iO5CpSW21TYa"
      },
      "outputs": [],
      "source": [
        "rand = random.randint(10,99)\n",
        "\n",
        "trainValDocs, testDocs, trainValTopics, testTopics = train_test_split(documents, topics, test_size=0.2, random_state=rand)\n",
        "trainDocs, valDocs, trainTopics, valTopcis = train_test_split(trainValDocs, trainValTopics, test_size=0.2, random_state=rand)\n",
        "print(trainDocs.shape)\n",
        "print(trainTopics.shape)\n",
        "print(valDocs.shape)\n",
        "print(valTopcis.shape)\n",
        "print(testDocs.shape)\n",
        "print(testTopics.shape)\n",
        "print(\"\\n_______________________\\n\")\n",
        "# Print the count of documents in each category for train, validation, and test sets\n",
        "for topic in favorite_topics:\n",
        "    length = len(trainTopics.index[trainTopics.applymap(lambda x: x == topic).any(axis=1)])\n",
        "    print(\"Category \", topic, \" counts in the train set:\", length)\n",
        "    length = len(valTopcis.index[valTopcis.applymap(lambda x: x == topic).any(axis=1)])\n",
        "    print(\"Category \", topic, \" counts in the validation set:\", length)\n",
        "    length = len(testTopics.index[testTopics.applymap(lambda x: x == topic).any(axis=1)])\n",
        "    print(\"Category \", topic, \" counts in the test set:\", length)\n",
        "    print(\"_______________________\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g2plhochKzbC"
      },
      "source": [
        "# **Save Split Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Heu4ieu5KzEd"
      },
      "outputs": [],
      "source": [
        "# trainDocs.to_hdf('/content/drive/My Drive/4topics_experiment.h5', key='trainDocs', mode='a')\n",
        "# trainTopics.to_hdf('/content/drive/My Drive/4topics_experiment.h5', key='trainTopics', mode='a')\n",
        "# valDocs.to_hdf('/content/drive/My Drive/4topics_experiment.h5', key='valDocs', mode='a')\n",
        "# valTopcis.to_hdf('/content/drive/My Drive/4topics_experiment.h5', key='valTopcis', mode='a')\n",
        "# testDocs.to_hdf('/content/drive/My Drive/4topics_experiment.h5', key='testDocs', mode='a')\n",
        "# testTopics.to_hdf('/content/drive/My Drive/4topics_experiment.h5', key='testTopics', mode='a')\n",
        "\n",
        "# trainDocs.to_csv('/content/drive/My Drive/trainDocs.csv', index=True)\n",
        "# trainTopics.to_csv('/content/drive/My Drive/trainTopics.csv', index=True)\n",
        "# valDocs.to_csv('/content/drive/My Drive/valDocs.csv', index=True)\n",
        "# valTopcis.to_csv('/content/drive/My Drive/valTopcis.csv', index=True)\n",
        "# testDocs.to_csv('/content/drive/My Drive/testDocs.csv', index=True)\n",
        "# testTopics.to_csv('/content/drive/My Drive/testTopics.csv', index=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_S5rj6lZLeVQ"
      },
      "source": [
        "# **Load Split Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vdd4xbfNLczb"
      },
      "outputs": [],
      "source": [
        "trainDocs = pd.read_hdf('/content/drive/My Drive/4topics_experiment.h5', key='trainDocs')\n",
        "trainTopics = pd.read_hdf('/content/drive/My Drive/4topics_experiment.h5', key='trainTopics')\n",
        "valDocs = pd.read_hdf('/content/drive/My Drive/4topics_experiment.h5', key='valDocs')\n",
        "valTopcis = pd.read_hdf('/content/drive/My Drive/4topics_experiment.h5', key='valTopcis')\n",
        "testDocs = pd.read_hdf('/content/drive/My Drive/4topics_experiment.h5', key='testDocs')\n",
        "testTopics = pd.read_hdf('/content/drive/My Drive/4topics_experiment.h5', key='testTopics')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# trainDocs = pd.read_csv('/content/drive/My Drive/trainDocs.csv')\n",
        "# valDocs = pd.read_csv('/content/drive/My Drive/valDocs.csv')\n",
        "# testDocs = pd.read_csv('/content/drive/My Drive/testDocs.csv')\n",
        "# trainTopics = pd.read_csv('/content/drive/My Drive/trainTopics.csv')\n",
        "# valTopcis = pd.read_csv('/content/drive/My Drive/valTopcis.csv')\n",
        "# testTopics = pd.read_csv('/content/drive/My Drive/testTopics.csv')\n",
        "\n",
        "# trainDocs.set_index('index', inplace=True)\n",
        "# valDocs.set_index('index', inplace=True)\n",
        "# testDocs.set_index('index', inplace=True)\n",
        "# trainTopics.set_index('index', inplace=True)\n",
        "# valTopcis.set_index('index', inplace=True)\n",
        "# testTopics.set_index('index', inplace=True)\n",
        "\n",
        "# trainDocs.index = trainDocs.index.astype(str)\n",
        "# valDocs.index = valDocs.index.astype(str)\n",
        "# testDocs.index = testDocs.index.astype(str)\n",
        "# trainTopics.index = trainTopics.index.astype(str)\n",
        "# valTopcis.index = valTopcis.index.astype(str)\n",
        "# testTopics.index = testTopics.index.astype(str)\n",
        "\n",
        "# trainDocs['documents'] = trainDocs['documents'].apply(ast.literal_eval)\n",
        "# valDocs['documents'] = valDocs['documents'].apply(ast.literal_eval)\n",
        "# testDocs['documents'] = testDocs['documents'].apply(ast.literal_eval)\n",
        "# trainTopics['documents'] = trainTopics['documents'].apply(ast.literal_eval)\n",
        "# valTopcis['documents'] = valTopcis['documents'].apply(ast.literal_eval)\n",
        "# testTopics['documents'] = testTopics['documents'].apply(ast.literal_eval)\n",
        "\n",
        "\n",
        "print(trainDocs.shape)\n",
        "print(trainTopics.shape)\n",
        "print(valDocs.shape)\n",
        "print(valTopcis.shape)\n",
        "print(testDocs.shape)\n",
        "print(testTopics.shape)\n",
        "print(\"\\n_______________________\\n\")\n",
        "# Print the count of documents in each category for train, validation, and test sets\n",
        "for topic in favorite_topics:\n",
        "    length = len(trainTopics.index[trainTopics.applymap(lambda x: x == topic).any(axis=1)])\n",
        "    print(\"Category \", topic, \" counts in the train set:\", length)\n",
        "    length = len(valTopcis.index[valTopcis.applymap(lambda x: x == topic).any(axis=1)])\n",
        "    print(\"Category \", topic, \" counts in the validation set:\", length)\n",
        "    length = len(testTopics.index[testTopics.applymap(lambda x: x == topic).any(axis=1)])\n",
        "    print(\"Category \", topic, \" counts in the test set:\", length)\n",
        "    print(\"_______________________\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLbh5gnGVYg7"
      },
      "source": [
        "# **Cosine Similarity Calculation And Vectorization**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X17YFMDqlneu"
      },
      "source": [
        "**Train Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R3mNuRxDYD5A"
      },
      "outputs": [],
      "source": [
        "tfidf_vectorizer = TfidfVectorizer(tokenizer=white_space_splitter, preprocessor=None, stop_words=None, max_df=1.0, min_df=1, max_features=None)\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(trainDocs['joined_tokens'])\n",
        "\n",
        "count_vectorizer = CountVectorizer(tokenizer=white_space_splitter, preprocessor=None, stop_words=None, max_df=1.0, min_df=1, max_features=None)\n",
        "tf_matrix = count_vectorizer.fit_transform(trainDocs['joined_tokens'])\n",
        "\n",
        "vocab_size = len(count_vectorizer.vocabulary_)\n",
        "\n",
        "# cosine_sim_score = cosine_similarity(tf_matrix.T)\n",
        "\n",
        "trainDocs['vectorized'] = trainDocs['preprocess'].apply(lambda lst: vectorize(tfidf_vectorizer.vocabulary_, lst))\n",
        "# trainDocs['vectorized'] = trainDocs['vectorized'].apply(lambda lst: remove_high_correlated_tokens(cosine_sim_score, lst))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CT2t1ppSEme6"
      },
      "outputs": [],
      "source": [
        "print(\"Number of tokens before cosine similarity: \" + str(vocab_size))\n",
        "print(\"Number of tokens after cosine similarity: \" + str(get_number_of_tokens(trainDocs['vectorized'])))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQzRFS94lqoO"
      },
      "source": [
        "**Vectorize Validation Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ipbBQ1PlrOf"
      },
      "outputs": [],
      "source": [
        "valDocs['vectorized'] = valDocs['preprocess'].apply(lambda lst: vectorize(tfidf_vectorizer.vocabulary_, lst))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OrWRE9vL3HJS"
      },
      "source": [
        "# **TF Feature**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kiEoGzKHEThM"
      },
      "source": [
        "**Train**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m9FVh5sDVyKG"
      },
      "outputs": [],
      "source": [
        "unique_terms = [token for tokenized_doc in trainDocs['vectorized'] for token in tokenized_doc]\n",
        "unique_terms = list(set(unique_terms))\n",
        "train_tf = pd.DataFrame(tf_matrix[:, unique_terms].toarray(), columns=unique_terms, index=trainDocs.index)\n",
        "trainDocs['tf'] = pd.DataFrame(train_tf.apply(create_list, axis=1)).iloc[:, 0].values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJF5kY9ESRJo"
      },
      "source": [
        "**Validation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CGE74GBzETGx"
      },
      "outputs": [],
      "source": [
        "val_tf_matrix = count_vectorizer.transform(valDocs['joined_tokens'])\n",
        "val_tf = pd.DataFrame(val_tf_matrix[:, unique_terms].toarray(), columns=unique_terms, index=valDocs.index)\n",
        "valDocs['tf'] = pd.DataFrame(val_tf.apply(create_list, axis=1)).iloc[:, 0].values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGtzwM1N9lsf"
      },
      "source": [
        "\n",
        "# **Terms Dictionary**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3a1X6Lku27u"
      },
      "source": [
        "**Term Topic Dictionary**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0G2OL7i09lUu"
      },
      "outputs": [],
      "source": [
        "vocab_size = get_number_of_tokens(trainDocs['vectorized'])\n",
        "token_topic_dict = pd.DataFrame(np.zeros(shape=(vocab_size, len(favorite_topics))), columns=range(len(favorite_topics)), index=unique_terms)\n",
        "\n",
        "for index in trainTopics.index:\n",
        "    topics_lst = trainTopics.loc[index]['topics_lst']\n",
        "    term_vector = trainDocs.loc[index]['vectorized']\n",
        "    for topic in topics_lst:\n",
        "        for term in term_vector:\n",
        "            token_topic_dict.loc[term][topic] += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "KkH8TXkfyXQ3"
      },
      "outputs": [],
      "source": [
        "filter_dictionary = input(\"Filter Dictionary?\")\n",
        "if filter_dictionary in [\"Y\", \"y\"]:\n",
        "    for topic in token_topic_dict.columns:\n",
        "        col_of_topic = token_topic_dict[topic] != 0\n",
        "        other_cols = token_topic_dict.drop(columns=topic).eq(0).all(axis=1)\n",
        "\n",
        "        # Combine the conditions\n",
        "        condition = col_of_topic & other_cols\n",
        "\n",
        "        # Mark the rows that meet the condition with 1\n",
        "        token_topic_dict['mark'] = np.where(condition, 1, 0)\n",
        "        topic_terms_index = token_topic_dict.sort_values(by=['mark',topic], ascending=False).iloc[100:].index\n",
        "        token_topic_dict.loc[topic_terms_index, topic] = 0\n",
        "    token_topic_dict.drop(columns='mark', inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kOIG80vZuykA"
      },
      "source": [
        "**Train Term Topic Weights**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vgYHC8kGx-4M"
      },
      "outputs": [],
      "source": [
        "trainTTW = pd.DataFrame(None, columns=range(len(favorite_topics)), index=trainDocs.index)\n",
        "\n",
        "for topic in list(token_topic_dict.columns):\n",
        "    trainTTW[topic] = trainDocs['vectorized'].apply(lambda lst: [token_topic_dict.loc[term][topic] for term in lst])\n",
        "trainTTW.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzI3r71tvS3q"
      },
      "source": [
        "**Padding**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "WB63UIAHMTtU",
        "outputId": "add37a3e-2c48-4767-ba25-ae36624e0207"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1279, 4)"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "for col in list(trainTTW.columns):\n",
        "    trainTTW[col] = trainTTW[col].apply(lambda lst: padding(lst, maximum_length))\n",
        "trainTTW.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uCMAG2ibwzIs"
      },
      "source": [
        "**Concatinate (Merge) Term Topic Weights**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L7-PN5pURGVh"
      },
      "outputs": [],
      "source": [
        "# Apply the function and create a new column\n",
        "columns = trainTTW.columns\n",
        "trainTTW['concatinated'] = trainTTW.apply(lambda lst: concatenate_arrays(columns, lst),  axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQRPvDwrmQ-6"
      },
      "source": [
        "**Validation Term Topic Weights**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7dohpVErmQsa"
      },
      "outputs": [],
      "source": [
        "valTTW = pd.DataFrame(None, columns=range(len(favorite_topics)), index=valDocs.index)\n",
        "\n",
        "for topic in list(token_topic_dict.columns):\n",
        "    valTTW[topic] = valDocs['vectorized'].apply(lambda lst: [token_topic_dict.loc[term][topic] if term in token_topic_dict.index else 0 for term in lst])\n",
        "valTTW.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7v5g0KOp30L"
      },
      "source": [
        "**Padding**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uPjkweoap3e7"
      },
      "outputs": [],
      "source": [
        "for col in list(valTTW.columns):\n",
        "    valTTW[col] = valTTW[col].apply(lambda lst: padding(lst, maximum_length))\n",
        "valTTW.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1MjQgWzLqE9a"
      },
      "source": [
        "**Concatinate (Merge) Term Topic Weights**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bhmvT8VVqEsf"
      },
      "outputs": [],
      "source": [
        "# Apply the function and create a new column\n",
        "columns = valTTW.columns\n",
        "valTTW['concatinated'] = valTTW.apply(lambda lst: concatenate_arrays(columns, lst),  axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-gRGdnG66yn3"
      },
      "source": [
        "# **TF-iDF**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MoGdDlzF6-Z1"
      },
      "source": [
        "**Train**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jQhADzK56yQH"
      },
      "outputs": [],
      "source": [
        "# unique terms calculated in the TF part\n",
        "train_tfidf = pd.DataFrame(tfidf_matrix[:, unique_terms].toarray(), columns=unique_terms, index=trainDocs.index)\n",
        "trainDocs['tfidf'] = pd.DataFrame(train_tfidf.apply(create_list, axis=1)).iloc[:, 0].values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZnonNS_OYR6L"
      },
      "source": [
        "**Test**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pjYtAaLBYRnE"
      },
      "outputs": [],
      "source": [
        "val_tfidf_matrix = tfidf_vectorizer.transform(valDocs['joined_tokens'])\n",
        "val_tfidf = pd.DataFrame(val_tfidf_matrix[:, unique_terms].toarray(), columns=unique_terms, index=valDocs.index)\n",
        "valDocs['tfidf'] = pd.DataFrame(val_tfidf.apply(create_list, axis=1)).iloc[:, 0].values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ecwBbTNOSpU4"
      },
      "source": [
        "\n",
        "# **PCA**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wf9NEs1YSqTg"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(trainDocs['vectorized'])\n",
        "# X_test_scaled = scaler.transform(df_test)\n",
        "\n",
        "pca = PCA(n_components=0.95)  # Retain 95% of the variance\n",
        "X_train_pca = pca.fit_transform(X_train_scaled)\n",
        "# X_test_pca = pca.transform(X_test_scaled)\n",
        "\n",
        "# 4. Examine the explained variance ratio\n",
        "explained_variance_ratio = pca.explained_variance_ratio_\n",
        "print(f'Explained variance ratio: {explained_variance_ratio}')\n",
        "print(f'Sum of explained variance ratio: {sum(explained_variance_ratio)}')\n",
        "\n",
        "# Optional: Convert PCA results back to DataFrame for easier handling\n",
        "df_train_pca = pd.DataFrame(X_train_pca, columns=[f'PC{i+1}' for i in range(X_train_pca.shape[1])])\n",
        "# df_test_pca = pd.DataFrame(X_test_pca, columns=[f'PC{i+1}' for i in range(X_test_pca.shape[1])])\n",
        "\n",
        "# Output the transformed data\n",
        "print(df_train_pca.head())\n",
        "# print(df_test_pca.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HxtUnfMh_9IY"
      },
      "outputs": [],
      "source": [
        "components = pca.components_\n",
        "\n",
        "# To find the top n features contributing to each component\n",
        "n = 10  # Example: top 1 features\n",
        "selected_features = []\n",
        "for i, component in enumerate(components):\n",
        "    # Get indices of the top n absolute loadings\n",
        "    indices = np.argsort(np.abs(component))[-n:]\n",
        "    selected_features.append(indices)\n",
        "    # print(f\"Top {n} features for principal component {i+1}: {indices}\")\n",
        "\n",
        "# Optional: Flatten the list if you want a single set of unique feature indices\n",
        "selected_features_flat = np.unique(np.concatenate(selected_features))\n",
        "selected_features_flat.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zIzc020gC736"
      },
      "outputs": [],
      "source": [
        "df_train_pca"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4xq4dSl5xV_x"
      },
      "source": [
        "# Token Tuples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8k622ETXx6zv"
      },
      "source": [
        "**Train Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KvqaNvaAxUhh",
        "outputId": "d336eb6f-28b1-4e4c-87ea-c93946c8c005"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tuple_2 unique tokens: 47929\n"
          ]
        }
      ],
      "source": [
        "# pairs of terms (size 2)\n",
        "trainDocs['tuple_2'] = trainDocs['vectorized'].apply(lambda lst: make_tuple(lst, 2))\n",
        "# tuple of size 3\n",
        "# trainDocs['tuple_3'] = trainDocs['vectorized'].apply(lambda lst: make_tuple(lst, 3))\n",
        "\n",
        "# get list of unique tokens\n",
        "unique_token_pairs = get_unique_token_pairs(trainDocs['tuple_2'])\n",
        "print('Tuple_2 unique tokens: ' + str(len(unique_token_pairs)))\n",
        "\n",
        "# get list of unique tokens\n",
        "# unique_token_pairs_3 = get_unique_token_pairs(trainDocs['tuple_3'])\n",
        "# print('Tuple_3 unique tokens: ' + str(len(unique_token_pairs_3)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1d64-0fDx9PK"
      },
      "source": [
        "**Validation Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "OVIBl9Gcx_t6"
      },
      "outputs": [],
      "source": [
        "valDocs['tuple_2'] = valDocs['vectorized'].apply(lambda lst: make_tuple(lst, 2))\n",
        "# valDocs['tuple_3'] = valDocs['vectorized'].apply(lambda lst: make_tuple(lst, 3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-JlshDBtWeS8"
      },
      "source": [
        "\n",
        "# Count the number of Tuples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k2Tzp-ReWrWf"
      },
      "source": [
        "**Train Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "SoYJ1pTvWXqs"
      },
      "outputs": [],
      "source": [
        "# Tuple Size 2\n",
        "pair_token_counts = []\n",
        "for pair in unique_token_pairs:\n",
        "    pair_token_counts.append(trainDocs['tuple_2'].apply(lambda lst: get_token_pairs_count(pair, lst)).to_numpy())\n",
        "\n",
        "# create a DataFrame of zeros with the tokens as column names\n",
        "pair_tokens_df = pd.DataFrame(pair_token_counts).T\n",
        "pair_tokens_df.index = trainDocs.index\n",
        "pair_tokens_df.columns = unique_token_pairs\n",
        "\n",
        "# Tuple Size 3\n",
        "# pair_3_token_counts = []\n",
        "# for pair in unique_token_pairs_3:\n",
        "#     pair_3_token_counts.append(trainDocs['tuple_3'].apply(lambda lst: get_token_pairs_count(pair, lst)).to_numpy())\n",
        "\n",
        "# create a DataFrame of zeros with the tokens as column names\n",
        "# pair_3_tokens_df = pd.DataFrame(pair_3_token_counts).T\n",
        "# pair_3_tokens_df.index = trainDocs.index\n",
        "# pair_3_tokens_df.columns = unique_token_pairs_3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5ZvfYq3WvBN"
      },
      "source": [
        "**Validation Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "P8mkGtF-Woj4"
      },
      "outputs": [],
      "source": [
        "pair_token_counts = []\n",
        "for pair in unique_token_pairs:\n",
        "    pair_token_counts.append(valDocs['tuple_2'].apply(lambda lst: get_token_pairs_count(pair, lst)).to_numpy())\n",
        "\n",
        "# create a DataFrame of zeros with the tokens as column names\n",
        "val_pair_tokens_df = pd.DataFrame(pair_token_counts).T\n",
        "val_pair_tokens_df.index = valDocs.index\n",
        "val_pair_tokens_df.columns = unique_token_pairs\n",
        "\n",
        "\n",
        "# Tuple Size 3\n",
        "# pair_3_token_counts = []\n",
        "# for pair in unique_token_pairs_3:\n",
        "#     pair_3_token_counts.append(valDocs['tuple_3'].apply(lambda lst: get_token_pairs_count(pair, lst)).to_numpy())\n",
        "\n",
        "# create a DataFrame of zeros with the tokens as column names\n",
        "# val_pair_3_tokens_df = pd.DataFrame(pair_3_token_counts).T\n",
        "# val_pair_3_tokens_df.index = valDocs.index\n",
        "# val_pair_3_tokens_df.columns = unique_token_pairs_3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mTgq1LFo4VI"
      },
      "source": [
        "# Feature Selection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "obI0jSFGqwze"
      },
      "source": [
        "**Train Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wVqILc0Fo3-t",
        "outputId": "3ccaaa66-231b-4cc1-e040-f90d4c2bd1ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No. of unique tuples-2 (features):4792\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-48-7ea83a993ba8>:8: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  train_selected_pairs_df['joined'] = train_selected_pairs_df.apply(create_list, axis=1)\n"
          ]
        }
      ],
      "source": [
        "# Tuple 2\n",
        "topics_array=trainTopics['topics_lst'].apply(lambda lst: lst[0]).values\n",
        "\n",
        "selected_pairs = feature_selection_rfe(pair_tokens_df, topics_array)\n",
        "print(\"No. of unique tuples-2 (features):\" + str(len(selected_pairs)))\n",
        "\n",
        "train_selected_pairs_df = pair_tokens_df[selected_pairs]\n",
        "train_selected_pairs_df['joined'] = train_selected_pairs_df.apply(create_list, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "xtPSxCUsgfkb"
      },
      "outputs": [],
      "source": [
        "# tuple 3\n",
        "# selected_pairs_3 = feature_selection_rfe(pair_3_tokens_df, topics_array)\n",
        "# print(\"No. of unique tuples-3 (features):\" + str(len(selected_pairs_3)))\n",
        "\n",
        "# train_selected_pairs_3_df = pair_3_tokens_df[selected_pairs_3]\n",
        "# train_selected_pairs_3_df['joined'] = train_selected_pairs_3_df.apply(create_list, axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILV63fYGKgWs"
      },
      "source": [
        "**Validation Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dUplRwjDq22G",
        "outputId": "4c01e4c6-b248-4de7-a869-b9bf277f9287"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-50-bffb282f83cb>:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  val_selected_pairs_df['joined'] = val_selected_pairs_df.apply(create_list, axis=1)\n"
          ]
        }
      ],
      "source": [
        "# Tuple 2\n",
        "val_selected_pairs_df = pd.DataFrame(0, index=valDocs.index, columns=selected_pairs, dtype=int)\n",
        "for col in selected_pairs:\n",
        "    if col in val_pair_tokens_df.columns:\n",
        "        val_selected_pairs_df[col] = val_pair_tokens_df[col]\n",
        "\n",
        "val_selected_pairs_df['joined'] = val_selected_pairs_df.apply(create_list, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "90sriiZXrfS3"
      },
      "outputs": [],
      "source": [
        "# Tuple 3\n",
        "# val_selected_pairs_3_df = pd.DataFrame(0, index=valDocs.index, columns=selected_pairs_3, dtype=int)\n",
        "# for col in selected_pairs_3:\n",
        "#     if col in val_pair_3_tokens_df.columns:\n",
        "#         val_selected_pairs_3_df[col] = val_pair_3_tokens_df[col]\n",
        "\n",
        "# val_selected_pairs_3_df['joined'] = val_selected_pairs_3_df.apply(create_list, axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3McFDJP3-RLL"
      },
      "source": [
        "# POS Tagging"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ci3hMOTnmUmr"
      },
      "source": [
        "**Train**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "61sguY83-Q-C"
      },
      "outputs": [],
      "source": [
        "trainDocs['pos_tag'] = trainDocs['preprocess'].apply(lambda lst: lemmatize(lst))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BIFumiS_mXRQ"
      },
      "source": [
        "**Validation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "EJd4owdumXzM"
      },
      "outputs": [],
      "source": [
        "valDocs['pos_tag'] = valDocs['preprocess'].apply(lambda lst: lemmatize(lst))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TS8FntbK_tEE"
      },
      "source": [
        "# Padding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKGX3poKmlmy"
      },
      "source": [
        "**Train Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "oklFR2ns_rN_"
      },
      "outputs": [],
      "source": [
        "trainDocs['token_padded'] = trainDocs['vectorized'].apply(lambda lst: padding(lst, maximum_length))\n",
        "trainDocs['pos_padded'] = trainDocs['pos_tag'].apply(lambda lst: padding(lst, maximum_length))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wsRlqysymoMl"
      },
      "source": [
        "**Validation Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "m3T5JXLRmoA7"
      },
      "outputs": [],
      "source": [
        "valDocs['token_padded'] = valDocs['vectorized'].apply(lambda lst: padding(lst, maximum_length))\n",
        "valDocs['pos_padded'] = valDocs['pos_tag'].apply(lambda lst: padding(lst, maximum_length))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BKzCk13OU5f4"
      },
      "source": [
        "# **GloVe Embedding**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "YIFcY-TXU4Ik"
      },
      "outputs": [],
      "source": [
        "embedding_matrix = GloVe_embedding(count_vectorizer.vocabulary_, vocab_size, embedding_dim)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4SsXrx3m8GC"
      },
      "source": [
        "# Neural Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A0l4WCCLm1o7"
      },
      "source": [
        "**Train Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "yTp8s5caeNXS"
      },
      "outputs": [],
      "source": [
        "trainDocs_X = np.array(trainDocs['token_padded'].tolist())\n",
        "trainPos_X = np.array(trainDocs['pos_padded'].tolist())\n",
        "trainTf_X = np.array(trainDocs['tf'].tolist())\n",
        "trainTfidf_X = np.array(trainDocs['tfidf'].tolist())\n",
        "trainTermDict_X = np.array(trainTTW['concatinated'].tolist())\n",
        "trainPairs_X =  np.array(train_selected_pairs_df['joined'].tolist())\n",
        "# trainPairs_3_X =  np.array(train_selected_pairs_3_df['joined'].tolist())\n",
        "train_Y = np.array(trainTopics['one_hot'].tolist())\n",
        "\n",
        "# convert pairs into np.array\n",
        "trainDocs_tuple_2 = np.array([np.array(pair) for doc in trainDocs['tuple_2'] for pair in doc])\n",
        "# trainDocs_tuple_3 = np.array([np.array(pair) for doc in trainDocs['tuple_3'] for pair in doc])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TzFKIKKom304"
      },
      "source": [
        "**Validation Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "5Dr4yBlCm3ld"
      },
      "outputs": [],
      "source": [
        "valDocs_X = np.array(valDocs['token_padded'].tolist())\n",
        "valPos_X = np.array(valDocs['pos_padded'].tolist())\n",
        "valTf_X = np.array(valDocs['tf'].tolist())\n",
        "valTfidf_X = np.array(valDocs['tfidf'].tolist())\n",
        "valTermDict_X = np.array(valTTW['concatinated'].tolist())\n",
        "valPairs_X =  np.array(val_selected_pairs_df['joined'].tolist())\n",
        "# valPairs_3_X =  np.array(val_selected_pairs_3_df['joined'].tolist())\n",
        "val_Y = np.array(valTopcis['one_hot'].tolist())\n",
        "\n",
        "# convert pairs into np.array\n",
        "valDocs_tuple_2 = np.array([np.array(pair) for doc in valDocs['tuple_2'] for pair in doc])\n",
        "# valDocs_tuple_3 = np.array([np.array(pair) for doc in valDocs['tuple_3'] for pair in doc])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJRimjYJnGqf"
      },
      "source": [
        "**Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rXzdfEipIDk3",
        "outputId": "7a767341-e80c-4e6b-d5fa-795263f5dbb9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "40/40 [==============================] - 391s 10s/step - loss: 1.0436 - accuracy: 0.5270 - val_loss: 1.0950 - val_accuracy: 0.2844\n",
            "Epoch 2/10\n",
            "40/40 [==============================] - 370s 9s/step - loss: 1.0123 - accuracy: 0.5434 - val_loss: 1.0521 - val_accuracy: 0.5750\n",
            "Epoch 3/10\n",
            "40/40 [==============================] - 379s 9s/step - loss: 0.9265 - accuracy: 0.5817 - val_loss: 0.9318 - val_accuracy: 0.6094\n",
            "Epoch 4/10\n",
            "40/40 [==============================] - 380s 10s/step - loss: 0.7931 - accuracy: 0.6865 - val_loss: 0.8822 - val_accuracy: 0.6438\n",
            "Epoch 5/10\n",
            "11/40 [=======>......................] - ETA: 4:31 - loss: 0.7280 - accuracy: 0.6875"
          ]
        }
      ],
      "source": [
        "# main text input\n",
        "# text_input = Input(shape=(maximum_length,))\n",
        "# embedding_layer = Embedding(input_dim=vocab_size, output_dim=embedding_dim)(text_input)\n",
        "# lstm_layer = LSTM(units=512)(embedding_layer)\n",
        "\n",
        "text_input = Input(shape=(maximum_length,))\n",
        "embedding_layer = Embedding(input_dim=vocab_size,\n",
        "                            output_dim=embedding_dim,\n",
        "                            weights=[embedding_matrix],\n",
        "                            input_length=maximum_length,\n",
        "                            trainable=False)(text_input)\n",
        "lstm_layer = LSTM(units=1024)(embedding_layer)\n",
        "\n",
        "\n",
        "# POS tags input for extra features\n",
        "# pos_layer_input = Input(shape=(num_extra_features,))\n",
        "# dense_pos_layer = Dense(512, activation='relu')(pos_layer_input)\n",
        "\n",
        "# TF input for extra features\n",
        "# num_unique_terms = len(unique_terms)\n",
        "# tf_layer_input = Input(shape=(num_unique_terms,))\n",
        "# dense_tf_layer = Dense(512, activation='relu')(tf_layer_input)\n",
        "\n",
        "# TFiDF input for extra features\n",
        "# num_unique_terms = len(unique_terms)\n",
        "# tfidf_layer_input = Input(shape=(num_unique_terms,))\n",
        "# dense_tfidf_layer = Dense(512, activation='relu')(tfidf_layer_input)\n",
        "\n",
        "# # Dictionary of terms layer\n",
        "# doc_term_dic_length = 4 * maximum_length\n",
        "# term_dict_layer_input = Input(shape=(doc_term_dic_length,))\n",
        "# dense_term_dict_layer = Dense(512, activation='relu')(term_dict_layer_input)\n",
        "\n",
        "# # pairs input for extra features\n",
        "# pair_layer_input = Input(shape=(len(selected_pairs),))\n",
        "# dense_pair_layer = Dense(512, activation='relu')(pair_layer_input)\n",
        "\n",
        "# # pairs input for extra features\n",
        "# pair_3_layer_input = Input(shape=(len(selected_pairs_3),))\n",
        "# dense_pair_3_layer = Dense(512, activation='relu')(pair_3_layer_input)\n",
        "\n",
        "# # pair terms (tuples size 2) text input\n",
        "# text_pairs_input = Input(shape=(None,2))\n",
        "# text_pairs_flatten_layer = Flatten()(text_pairs_input)\n",
        "# dense_text_pairs_layer = Dense(256, activation='relu')(text_pairs_flatten_layer)\n",
        "\n",
        "# Merge the outputs of the main text input and auxiliary input\n",
        "# merged = concatenate([lstm_layer, dense_pos_layer, dense_pair_layer])\n",
        "# merged = concatenate([lstm_layer, dense_pos_layer, dense_tf_layer, dense_tfidf_layer, dense_term_dict_layer, dense_pair_layer])\n",
        "\n",
        "# Additional layers for further processing\n",
        "# dense_layer_1 = Dense(2048, activation='relu')()\n",
        "dense_layer_1 = Dense(128, activation='relu')(lstm_layer)\n",
        "dense_layer_2 = Dense(64, activation='relu')(dense_layer_1)\n",
        "dense_layer_3 = Dense(32, activation='relu')(dense_layer_2)\n",
        "output_layer = Dense(num_classes, activation='softmax')(dense_layer_3)\n",
        "\n",
        "\n",
        "# Define the model\n",
        "# model = Model(inputs=[text_input], outputs=output_layer)\n",
        "# model = Model(inputs=[tfidf_layer_input, term_dict_layer_input, pair_layer_input], outputs=output_layer)\n",
        "model = Model(inputs=[text_input], outputs=output_layer)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "# model.fit([trainDocs['token_padded'].tolist(), trainDocs['pos_padded'].tolist()], trainTopics['one_hot'].tolist(), epochs=10, batch_size=32, validation_split=0.2)\n",
        "# model.fit({'text_input': trainDocs['token_padded'], 'extra_input': trainDocs['pos_padded']},\n",
        "#           {'output': trainTopics['one_hot']},\n",
        "#           epochs=10, batch_size=32, validation_split=0.2)\n",
        "# model.fit([trainDocs_X], train_Y, epochs=10, batch_size=32, validation_data=([valDocs_X], val_Y))\n",
        "# model.fit([trainTfidf_X, trainTermDict_X, trainPairs_X], train_Y, epochs=10, batch_size=32, validation_data=([valTfidf_X, valTermDict_X, valPairs_X], val_Y))\n",
        "model.fit([trainDocs_X], train_Y, epochs=10, batch_size=32, validation_data=([valDocs_X], val_Y))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FnpnTrneV5a1"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rH3t7Ao4V5KX",
        "outputId": "459928b3-4eb0-4288-be17-01d24177fa31"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10/10 [==============================] - 7s 661ms/step\n",
            "F1 Score: [0.6848249  0.         0.         0.86880466]\n",
            "Precision: [0.56050955 0.         0.         0.91411043]\n",
            "Recall: [0.88       0.         0.         0.82777778]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "# val_predictions = model.predict([valTfidf_X, valTermDict_X, valPairs_X])\n",
        "val_predictions = model.predict([valDocs_X])\n",
        "\n",
        "# Convert predictions and true labels to class labels\n",
        "val_true_labels = np.argmax(val_Y, axis=1)\n",
        "val_pred_labels = np.argmax(val_predictions, axis=1)\n",
        "\n",
        "# Calculate precision, recall, and F1 score\n",
        "precision = precision_score(val_true_labels, val_pred_labels, average=None)\n",
        "recall = recall_score(val_true_labels, val_pred_labels, average=None)\n",
        "f1 = f1_score(val_true_labels, val_pred_labels, average=None)\n",
        "\n",
        "print(f'F1 Score: {f1}')\n",
        "print(f'Precision: {precision}')\n",
        "print(f'Recall: {recall}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o2uBDhkMwtti"
      },
      "source": [
        "# Test Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SGfQIOoAwqwp"
      },
      "source": [
        "**Vectorize Test Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j8hbj2QTOisn"
      },
      "outputs": [],
      "source": [
        "testDocs['vectorized'] = testDocs['preprocess'].apply(lambda lst: vectorize(tfidf_vectorizer.vocabulary_, lst))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5BQNGP59PW3D"
      },
      "outputs": [],
      "source": [
        "test_tf_matrix = count_vectorizer.transform(testDocs['joined_tokens'])\n",
        "test_tf = pd.DataFrame(test_tf_matrix[:, unique_terms].toarray(), columns=unique_terms, index=testDocs.index)\n",
        "testDocs['tf'] = pd.DataFrame(test_tf.apply(create_list, axis=1)).iloc[:, 0].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7e8yBaUCbhIA"
      },
      "outputs": [],
      "source": [
        "test_tfidf_matrix = tfidf_vectorizer.transform(testDocs['joined_tokens'])\n",
        "test_tfidf = pd.DataFrame(test_tfidf_matrix[:, unique_terms].toarray(), columns=unique_terms, index=testDocs.index)\n",
        "testDocs['tfidf'] = pd.DataFrame(test_tfidf.apply(create_list, axis=1)).iloc[:, 0].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PPRK4u5MxkVl"
      },
      "outputs": [],
      "source": [
        "testTTW = pd.DataFrame(None, columns=range(len(favorite_topics)), index=testDocs.index)\n",
        "\n",
        "for topic in list(token_topic_dict.columns):\n",
        "    testTTW[topic] = testDocs['vectorized'].apply(lambda lst: [token_topic_dict.loc[term][topic] if term in token_topic_dict.index else 0 for term in lst])\n",
        "\n",
        "for col in list(testTTW.columns):\n",
        "    testTTW[col] = testTTW[col].apply(lambda lst: padding(lst, maximum_length))\n",
        "\n",
        "# Apply the function and create a new column\n",
        "columns = testTTW.columns\n",
        "testTTW['concatinated'] = testTTW.apply(lambda lst: concatenate_arrays(columns, lst),  axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t7vcssN6oigk"
      },
      "outputs": [],
      "source": [
        "testDocs['tuple_2'] = testDocs['vectorized'].apply(lambda lst: make_tuple(lst, 2))\n",
        "\n",
        "pair_token_counts = []\n",
        "for pair in unique_token_pairs:\n",
        "    pair_token_counts.append(testDocs['tuple_2'].apply(lambda lst: get_token_pairs_count(pair, lst)).to_numpy())\n",
        "\n",
        "# create a DataFrame of zeros with the tokens as column names\n",
        "test_pair_tokens_df = pd.DataFrame(pair_token_counts).T\n",
        "test_pair_tokens_df.index = testDocs.index\n",
        "test_pair_tokens_df.columns = unique_token_pairs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bXJtYGWDzxCq"
      },
      "outputs": [],
      "source": [
        "# testDocs['tuple_3'] = testDocs['vectorized'].apply(lambda lst: make_tuple(lst, 3))\n",
        "\n",
        "# pair_3_token_counts = []\n",
        "# for pair in unique_token_pairs:\n",
        "#     pair_3_token_counts.append(testDocs['tuple_3'].apply(lambda lst: get_token_pairs_count(pair, lst)).to_numpy())\n",
        "\n",
        "# # create a DataFrame of zeros with the tokens as column names\n",
        "# test_pair_3_tokens_df = pd.DataFrame(pair_3_token_counts).T\n",
        "# test_pair_3_tokens_df.index = testDocs.index\n",
        "# test_pair_3_tokens_df.columns = unique_token_pairs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5NX8KuScWJGz",
        "outputId": "e3d055a3-4a21-49f1-8ba8-3f30309bfdc9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-85-54723e366200>:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  test_selected_pairs_df['joined'] = test_selected_pairs_df.apply(create_list, axis=1)\n"
          ]
        }
      ],
      "source": [
        "test_selected_pairs_df = pd.DataFrame(0, index=testDocs.index, columns=selected_pairs, dtype=int)\n",
        "for col in selected_pairs:\n",
        "    if col in test_pair_tokens_df.columns:\n",
        "        test_selected_pairs_df[col] = test_pair_tokens_df[col]\n",
        "\n",
        "test_selected_pairs_df['joined'] = test_selected_pairs_df.apply(create_list, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "unpaUdj90ygz"
      },
      "outputs": [],
      "source": [
        "# test_selected_pairs_3_df = pd.DataFrame(0, index=testDocs.index, columns=selected_pairs_3, dtype=int)\n",
        "# for col in selected_pairs_3:\n",
        "#     if col in test_pair_3_tokens_df.columns:\n",
        "#         test_selected_pairs_3_df[col] = test_pair_3_tokens_df[col]\n",
        "\n",
        "# test_selected_pairs_3_df['joined'] = test_selected_pairs_3_df.apply(create_list, axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EcdbdQo5xH38"
      },
      "source": [
        "**POS Tagging Test Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NRN5PXx-xIae"
      },
      "outputs": [],
      "source": [
        "testDocs['pos_tag'] = testDocs['preprocess'].apply(lambda lst: lemmatize(lst))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0117O04xmrw"
      },
      "source": [
        "**Padding Test Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AgI33XicxkiO"
      },
      "outputs": [],
      "source": [
        "testDocs['token_padded'] = testDocs['vectorized'].apply(lambda lst: padding(lst, maximum_length))\n",
        "testDocs['pos_padded'] = testDocs['pos_tag'].apply(lambda lst: padding(lst, maximum_length))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WIZwc1mTyJvv"
      },
      "source": [
        "**Dataframe to Array**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A6zlRIfPyJiK"
      },
      "outputs": [],
      "source": [
        "testDocs_X = np.array(testDocs['token_padded'].tolist())\n",
        "testPos_X = np.array(testDocs['pos_padded'].tolist())\n",
        "testTf_X = np.array(testDocs['tf'].tolist())\n",
        "testTfidf_X = np.array(testDocs['tfidf'].tolist())\n",
        "testTermDict_X = np.array(testTTW['concatinated'].tolist())\n",
        "testPairs_X =  np.array(test_selected_pairs_df['joined'].tolist())\n",
        "# testPairs_3_X =  np.array(test_selected_pairs_3_df['joined'].tolist())\n",
        "test_y = np.array(testTopics['one_hot'].tolist())\n",
        "label_y = np.argmax(test_y, axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-dV3-TL9x8cJ"
      },
      "source": [
        "**Evaluate Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0_UArY-Gx8Mh",
        "outputId": "f295c03d-84dc-4df6-80a1-4bcb1fe55606"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "13/13 [==============================] - 15s 1s/step - loss: 0.9217 - accuracy: 0.6975\n",
            "Test Loss: 0.9217197895050049\n",
            "Test Accuracy: 0.6974999904632568\n"
          ]
        }
      ],
      "source": [
        "# evaluation_results = model.evaluate([testDocs_X], test_y)\n",
        "# evaluation_results = model.evaluate([testTfidf_X, testTermDict_X, testPairs_X], test_y)\n",
        "evaluation_results = model.evaluate([testDocs_X], test_y)\n",
        "print(\"Test Loss:\", evaluation_results[0])\n",
        "print(\"Test Accuracy:\", evaluation_results[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LCvuxHsiHiaA"
      },
      "source": [
        "**Prediction, Recall, F1-Score**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lK2bR1zx7Fwu",
        "outputId": "67880c21-62b6-404c-a4ae-29db29062d16"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "13/13 [==============================] - 7s 383ms/step\n",
            "F1-score per class: [0.67574932 0.32       0.41726619 0.76551724]\n",
            "Precision per class: [0.51882845 0.33333333 0.28712871 0.63428571]\n",
            "Recall per class: [0.96875    0.30769231 0.76315789 0.96521739]\n",
            "Micro F-score: 0.6822682268226823\n",
            "Macro F-score: 0.54463318680769\n"
          ]
        }
      ],
      "source": [
        "# predictions = pd.DataFrame(model.predict([testDocs_X]))\n",
        "predictions = pd.DataFrame(model.predict([testDocs_X]))\n",
        "# predictions = pd.DataFrame(model.predict([testPos_X, testTf_X, testTfidf_X, testTermDict_X, testPairs_X]))\n",
        "predictions['classes_probs'] = predictions.apply(lambda row: list(row), axis=1)\n",
        "# predictions['pred_label'] = predictions['classes_probs'].apply(lambda lst: np.argmax(lst))\n",
        "predictions['pred_label'] = predictions['classes_probs'].apply(lambda lst: (np.array(lst) >= 0.05).astype(int))\n",
        "\n",
        "predicted_y = np.array(predictions['pred_label'].tolist())\n",
        "\n",
        "precision_per_class = precision_score(test_y, predicted_y, average=None)\n",
        "recall_per_class = recall_score(test_y, predicted_y, average=None)\n",
        "f1score_per_class = f1_score(test_y, predicted_y, average=None)\n",
        "\n",
        "print(\"F1-score per class:\", f1score_per_class)\n",
        "print(\"Precision per class:\", precision_per_class)\n",
        "print(\"Recall per class:\", recall_per_class)\n",
        "\n",
        "micro_f_score = f1_score(test_y, predicted_y, average='micro')\n",
        "macro_f_score = f1_score(test_y, predicted_y, average='macro')\n",
        "\n",
        "print(\"Micro F-score:\", micro_f_score)\n",
        "print(\"Macro F-score:\", macro_f_score)\n",
        "\n",
        "# confusion_mat = confusion_matrix(test_y, predicted_y)\n",
        "# print(\"Confusion Matrix:\")\n",
        "# print(confusion_mat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CQPQIcjHgSFI"
      },
      "outputs": [],
      "source": [
        "model.save('NN_Ver3_Exp1.keras')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5zwgv3FjCeYH"
      },
      "outputs": [],
      "source": [
        "for i in range(test_y.shape[1]):\n",
        "    precision_per_class = precision_score(test_y[:, i], predicted_y[:, i], average=None)\n",
        "    recall_per_class = recall_score(test_y[:, i], predicted_y[:, i], average=None)\n",
        "\n",
        "    print(\"Precision per class:\", precision_per_class)\n",
        "    print(\"Recall per class:\", recall_per_class)\n",
        "\n",
        "    cm = confusion_matrix(test_y[:, i], predicted_y[:, i])\n",
        "    print(f\"Confusion Matrix for label {i}:\")\n",
        "    print(cm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bnq_kq-iAQ-E"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# F-scores for 6 experiments\n",
        "f_scores = [0.414, 0.601, 0.72, 0.194, 0.215, 0.3]\n",
        "experiments = ['Exp1', 'Exp2', 'Exp3', 'Exp4', 'Exp5', 'Exp6']\n",
        "\n",
        "# Create a bar plot\n",
        "fig, ax = plt.subplots()\n",
        "bars = ax.bar(experiments, f_scores, color=['blue', 'green', 'red', 'purple', 'orange', 'cyan'])\n",
        "\n",
        "# Add labels and title\n",
        "ax.set_xlabel('Experiment')\n",
        "ax.set_ylabel('F-score')\n",
        "ax.set_title('F-scores of Different Experiments')\n",
        "\n",
        "# Create a legend\n",
        "legend_labels = [f'Experiment {i+1}' for i in range(len(experiments))]\n",
        "ax.legend(bars, legend_labels)\n",
        "\n",
        "# Display the plot\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RSz8-_lvTMFx"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# F-scores for 6 experiments (main columns)\n",
        "f_scores = values = [0.39, 0.38, 0.68, 0.49, 0.62, 0.55]\n",
        "\n",
        "experiments = ['Exp1', 'Exp2', 'Exp3', 'Exp4', 'Exp5', 'Exp6']\n",
        "\n",
        "# Sub-columns data for each experiment\n",
        "sub_scores = [\n",
        "    [0.51, 0.1, 0.26, 0.70],\n",
        "    [0.54, 0.0, 0.26, 0.72],\n",
        "    [0.79, 0.55, 0.47, 0.92],\n",
        "    [0.64, 0.0, 0.5, 0.80],\n",
        "    [0.70, 0.47, 0.37, 0.93],\n",
        "    [0.71, 0.47, 0.27, 0.77],\n",
        "]\n",
        "\n",
        "# Define width of bars\n",
        "bar_width = 0.1\n",
        "\n",
        "# Define positions for bars\n",
        "main_bar_positions = np.arange(len(experiments))\n",
        "sub_bar_positions = [main_bar_positions + bar_width * (i + 1) for i in range(4)]\n",
        "\n",
        "# Create the plot\n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "# Plot main columns (background columns)\n",
        "main_bars = ax.bar(main_bar_positions, f_scores, color='grey', width=bar_width*4, label='Macro F-score')\n",
        "\n",
        "my_list = np.array([0, 1, 2, 3])\n",
        "\n",
        "# Plot sub-columns\n",
        "for i, sub_score in enumerate(sub_scores):\n",
        "    if i == 0:\n",
        "        sub_bars = ax.bar(main_bar_positions[i] + (bar_width * my_list), sub_score, width=bar_width, label=['acq', 'corn', 'crude', 'earn'], color=['green', 'red', 'purple', 'orange'])\n",
        "    else:\n",
        "        sub_bars = ax.bar(main_bar_positions[i] + (bar_width * my_list), sub_score, width=bar_width, color=['green', 'red', 'purple', 'orange'])\n",
        "\n",
        "# Add labels and title\n",
        "ax.set_xlabel('Experiment')\n",
        "ax.set_ylabel('F-score')\n",
        "ax.set_title('F-scores of Different Experiments')\n",
        "ax.set_xticks(main_bar_positions + bar_width * 1.5)\n",
        "ax.set_xticklabels(experiments)\n",
        "\n",
        "# Create a legend\n",
        "ax.legend(bbox_to_anchor=(1, 1), loc='upper left')\n",
        "\n",
        "plt.savefig('experiment_f_scores_NN4_3features.png', dpi=300, bbox_inches='tight')\n",
        "\n",
        "# Display the plot\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mlTgG2NCWHEu"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jtxuz3cD6UQb"
      },
      "source": [
        "# **Coherence**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BXLWksQAIKmy"
      },
      "outputs": [],
      "source": [
        "import gensim\n",
        "from gensim.corpora.dictionary import Dictionary\n",
        "from gensim.models.coherencemodel import CoherenceModel\n",
        "\n",
        "p_topics = np.argmax(predicted_y == 1, axis=1)\n",
        "p_topics = [[item] for item in p_topics]\n",
        "\n",
        "c_dictionary = Dictionary(testDocs['preprocess'])\n",
        "# c_dictionary\n",
        "\n",
        "c_corpus = [c_dictionary.doc2bow(doc) for doc in testDocs['preprocess']]\n",
        "# c_corpus\n",
        "\n",
        "# Create CoherenceModel\n",
        "cm = CoherenceModel(topics=p_topics, corpus=c_corpus, dictionary=c_dictionary, coherence='u_mass')\n",
        "cm.get_coherence_per_topic()\n",
        "# Get coherence value\n",
        "# coherence = cm.get_coherence()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7hyCRbFbn57V"
      },
      "source": [
        "# **Perplexity**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VPdQj0sqe6pH"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import log_loss\n",
        "\n",
        "pred_prob = predictions['classes_probs'].to_list()\n",
        "true_y = [[sublist[0]] for sublist in testTopics['topics_lst']]\n",
        "cross_entropy_loss = log_loss(np.array(true_y), np.array(pred_prob))\n",
        "perplexity = np.exp(cross_entropy_loss)\n",
        "perplexity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xxXu-cFQ59e8"
      },
      "outputs": [],
      "source": [
        "np.array(pred_prob)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wh9SPrk8otMV"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fnQu2pGi6S08"
      },
      "outputs": [],
      "source": [
        "# word_topic = np.zeros(shape=(vocab_size, len(favorite_topics)))\n",
        "\n",
        "# for doc_index in trainDocs.index:\n",
        "#     topic_list = trainTopics.loc[doc_index]['topics_lst']\n",
        "#     token_list = trainDocs.loc[doc_index]['vectorized']\n",
        "#     for token in token_list:\n",
        "#         for topic in topic_list:\n",
        "#             word_topic[token, topic] += 1\n",
        "\n",
        "# predicted_y\n",
        "\n",
        "# testDocs['preprocess']\n",
        "\n",
        "# import gensim\n",
        "# from gensim.corpora.dictionary import Dictionary\n",
        "# from gensim.models.coherencemodel import CoherenceModel\n",
        "# c_dictionary = Dictionary(testDocs['preprocess'])\n",
        "\n",
        "# c_corpus = [c_dictionary.doc2bow(doc) for doc in testDocs['preprocess']]\n",
        "# c_corpus\n",
        "\n",
        "# Create CoherenceModel\n",
        "# cm = CoherenceModel(topics=list_of_lists, corpus=c_corpus, dictionary=c_dictionary, coherence='u_mass')\n",
        "\n",
        "# # Get coherence value\n",
        "coherence = cm.get_coherence()\n",
        "\n",
        "# a = np.argmax(predicted_y == 1, axis=1)\n",
        "# list_of_lists = [[item] for item in a]\n",
        "# list_of_lists\n",
        "coherence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bdVz72S7JLPm"
      },
      "outputs": [],
      "source": [
        "list_of_lists"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vyKxqcHjLuSU"
      },
      "source": [
        "# **GloVe Test Section**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wSugRqEkMNJr",
        "outputId": "32bccbba-bb36-4c90-f34d-b6445b8bc8c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2024-06-24 09:57:18--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2024-06-24 09:57:18--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2024-06-24 09:57:18--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  5.01MB/s    in 2m 39s  \n",
            "\n",
            "2024-06-24 09:59:57 (5.18 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n",
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P--w6EwbMNgy"
      },
      "outputs": [],
      "source": [
        "# 2. Load GloVe embeddings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kySb8vaqPNzl",
        "outputId": "608aa57e-60ee-43a4-d90f-d20b91e87cba"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[[2, 3, 4, 1]]"
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer = Tokenizer(num_words=100)\n",
        "tokenizer.fit_on_texts(['My name is Ali Hossein new all you can do call me john PC security. I love Ali'])\n",
        "tokenizer.texts_to_sequences(['My name is Ali'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vSQi0Lm71XV-",
        "outputId": "f949064b-ca0b-45e9-d504-270ba73e6e5e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'ali': 1,\n",
              " 'my': 2,\n",
              " 'name': 3,\n",
              " 'is': 4,\n",
              " 'hossein': 5,\n",
              " 'new': 6,\n",
              " 'all': 7,\n",
              " 'you': 8,\n",
              " 'can': 9,\n",
              " 'do': 10,\n",
              " 'call': 11,\n",
              " 'me': 12,\n",
              " 'john': 13,\n",
              " 'pc': 14,\n",
              " 'security': 15,\n",
              " 'i': 16,\n",
              " 'love': 17}"
            ]
          },
          "execution_count": 55,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.word_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rusOcd7GH3PW"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QNVbw5ZyJurK",
        "outputId": "cadd1a88-44c2-47d2-dc59-868dca7b6955"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(7503, 100)"
            ]
          },
          "execution_count": 72,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "embedding_matrix.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "BLquA9meLtgc",
        "outputId": "1cc81fc5-b986-4985-d61b-c983bbe8dfd7"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'num_unique_terms' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-73-ce36504e3e19>\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# TF input for extra features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mtf_layer_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_unique_terms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mdense_tf_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf_layer_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'num_unique_terms' is not defined"
          ]
        }
      ],
      "source": [
        "\n",
        "text_input = Input(shape=(maximum_length,))\n",
        "embedding_layer = Embedding(input_dim=vocab_size,\n",
        "                            output_dim=embedding_dim,\n",
        "                            weights=[embedding_matrix],\n",
        "                            input_length=maximum_length,\n",
        "                            trainable=False)(text_input)\n",
        "lstm_layer = LSTM(units=512)(embedding_layer)\n",
        "\n",
        "# POS tags input for extra features\n",
        "pos_layer_input = Input(shape=(num_extra_features,))\n",
        "dense_pos_layer = Dense(512, activation='relu')(pos_layer_input)\n",
        "\n",
        "# TF input for extra features\n",
        "tf_layer_input = Input(shape=(num_unique_terms,))\n",
        "dense_tf_layer = Dense(512, activation='relu')(tf_layer_input)\n",
        "\n",
        "# TFiDF input for extra features\n",
        "tfidf_layer_input = Input(shape=(num_unique_terms,))\n",
        "dense_tfidf_layer = Dense(512, activation='relu')(tfidf_layer_input)\n",
        "\n",
        "# Dictionary of terms layer\n",
        "term_dict_layer_input = Input(shape=(doc_term_dic_length,))\n",
        "dense_term_dict_layer = Dense(512, activation='relu')(term_dict_layer_input)\n",
        "\n",
        "# Pairs input for extra features\n",
        "pair_layer_input = Input(shape=(len(selected_pairs),))\n",
        "dense_pair_layer = Dense(512, activation='relu')(pair_layer_input)\n",
        "\n",
        "# Merge the outputs of the main text input and auxiliary input\n",
        "merged = concatenate([lstm_layer, dense_pos_layer, dense_tf_layer, dense_tfidf_layer, dense_term_dict_layer, dense_pair_layer])\n",
        "\n",
        "# Additional layers for further processing\n",
        "dense_layer_1 = Dense(128, activation='relu')(merged)\n",
        "dense_layer_2 = Dense(64, activation='relu')(dense_layer_1)\n",
        "dense_layer_3 = Dense(32, activation='relu')(dense_layer_2)\n",
        "output_layer = Dense(num_classes, activation='softmax')(dense_layer_3)\n",
        "\n",
        "# Define the model\n",
        "model = Model(inputs=[text_input, pos_layer_input, tf_layer_input, tfidf_layer_input, term_dict_layer_input, pair_layer_input], outputs=output_layer)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit([trainDocs_X, trainPos_X, trainTf_X, trainTfidf_X, trainTermDict_X, trainPairs_X], train_Y, epochs=10, batch_size=32, validation_data=([valDocs_X, valPos_X, valTf_X, valTfidf_X, valTermDict_X, valPairs_X], val_Y))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "DPlHTYA05qWL",
        "MGSA5l1mN2I7",
        "h0sbrqiZyqre",
        "gyDYe-MKyrOA",
        "QmcFLPx8U_0e",
        "vBn_8r789YT3",
        "TAiG1vSQ-GlM",
        "rCxMcty3_XKr",
        "Kw8obdfhLNXt",
        "Dp_AzMU4LOkl",
        "757dPthIOnqu",
        "ALHuYuZN14eM",
        "n-GKyjGnjbaF",
        "YuGxOq79joCs",
        "iklnNbA8jomV",
        "g2plhochKzbC",
        "_S5rj6lZLeVQ",
        "OrWRE9vL3HJS",
        "-gRGdnG66yn3",
        "ecwBbTNOSpU4",
        "4xq4dSl5xV_x",
        "-JlshDBtWeS8",
        "1mTgq1LFo4VI",
        "3McFDJP3-RLL",
        "TS8FntbK_tEE"
      ],
      "provenance": [],
      "mount_file_id": "1jaMTsjpVepm_5tEp-o-hM3wo7Rr4IrDk",
      "authorship_tag": "ABX9TyO2cSE2ZXv4nfSbhJ5H+i1Y",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}